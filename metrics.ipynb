{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe_image as dfi\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact, IntSlider, Output, VBox, HTML as HTMLWidget, Button, HBox\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "import textstat\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "#from gsm8k import SYSTEM_PROMPT\n",
    "import textwrap\n",
    "from glob import glob\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    return text.split(\"####\")[-1].strip() if \"####\" in text else None\n",
    "\n",
    "def get_gsm8k_questions(split=\"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_gsm8k_questions(split = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = sorted(glob('lora_checkpoints/*/test_examples_fixed.json'))\n",
    "responses = {}\n",
    "for o in output_files:\n",
    "    with open(o) as f:\n",
    "        all_outputs = json.load(f)\n",
    "    assert len(all_outputs) == 6 or len(all_outputs) == 5, len(all_outputs)\n",
    "    responses[o.split('/')[1]] = all_outputs\n",
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_file = glob('lora_checkpoints/pretrained_examples_fixed.json')\n",
    "assert len(pretrained_file) == 1, len(pretrained_file)\n",
    "pretrained_file = pretrained_file[0]\n",
    "with open(pretrained_file) as f:\n",
    "    pretrained_outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(text: str) -> str:\n",
    "    return text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "\n",
    "def get_answer_rate(responses):\n",
    "    correct, wrong = 0, 0\n",
    "    for i, o in enumerate(responses):\n",
    "        try: \n",
    "            ans = int(get_answer(o))\n",
    "            if ans == int(dataset[i]['answer']):\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        except:\n",
    "            pass\n",
    "    return correct / (wrong + correct)\n",
    "\n",
    "def get_total_average_flesch_kincaid(responses) -> float:\n",
    "    scores = [textstat.flesch_kincaid_grade(r) for r in responses]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def flesch_kincaid_reward_func(responses) -> float:\n",
    "    scores = [textstat.flesch_kincaid_grade(r.split('<reasoning>')[-1].split('</reasoning>')[0]) for r in responses]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def get_average_length(responses) -> list[float]:\n",
    "    return sum([len(r) for r in responses]) / len(responses)\n",
    "\n",
    "def soft_format_reward_func(responses) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
    "    return sum([1 if match else 0 for match in matches])\n",
    "\n",
    "def has_reasoning(responses) -> list[float]:\n",
    "    return sum([1 if (\"<reasoning>\" in r and '</reasoning>' in r) else 0 for r in responses])\n",
    "\n",
    "def has_answer(responses) -> list[float]:\n",
    "    return sum([1 if (\"<answer>\" in r and \"</answer>\" in r) else 0 for r in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}\n",
    "for n, all_outputs in responses.items():\n",
    "    metrics = []\n",
    "    for outputs in all_outputs:\n",
    "        metrics.append({\n",
    "            \"correct answer\\n(when <answer> present)\": get_answer_rate(outputs),\n",
    "            \"average_flesch_kincaid\": get_total_average_flesch_kincaid(outputs),\n",
    "            \"reasoning flesch kincaid\": flesch_kincaid_reward_func(outputs),\n",
    "            \"average_length\": get_average_length(outputs),\n",
    "            \"soft_format_reward\": soft_format_reward_func(outputs),\n",
    "            \"has reasoning tokens\": has_reasoning(outputs),\n",
    "            \"has answer tokens\": has_answer(outputs),\n",
    "        })\n",
    "    all_metrics[n] = metrics\n",
    "\n",
    "pretrained_metrics = {\n",
    "    \"correct answer\\n(when <answer> present)\": get_answer_rate(pretrained_outputs),\n",
    "    \"average_flesch_kincaid\": get_total_average_flesch_kincaid(pretrained_outputs),\n",
    "    \"reasoning flesch kincaid\": flesch_kincaid_reward_func(pretrained_outputs),\n",
    "    \"average_length\": get_average_length(pretrained_outputs),\n",
    "    \"soft_format_reward\": soft_format_reward_func(pretrained_outputs),\n",
    "    \"has reasoning tokens\": has_reasoning(pretrained_outputs),\n",
    "    \"has answer tokens\": has_answer(pretrained_outputs),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "limits = {\n",
    "    \"correct answer\\n(when <answer> present)\" : [0, 1],\n",
    "    \"soft_format_reward\" : [0,100],\n",
    "    \"has reasoning tokens\": [0,100],\n",
    "    \"has answer tokens\": [0,100]\n",
    "}\n",
    "checkpoints = list(range(500, 3001, 500))\n",
    "fig, axarr = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axarr = axarr.flatten()\n",
    "cats = metrics[0].keys()\n",
    "colors = ['r', 'b', 'g', 'orange']\n",
    "\n",
    "\n",
    "# Collect handles for method lines only from the first subplot\n",
    "method_handles = []\n",
    "\n",
    "for ax_idx, (ax, cat) in enumerate(zip(axarr, cats)):\n",
    "    for i, (name, metrics) in enumerate(all_metrics.items()):\n",
    "        y = [metrics[i][cat] for i in range(0, len(metrics))]\n",
    "        line, = ax.plot(checkpoints[:len(metrics)], y, marker='o', color=colors[i], label=name)\n",
    "        if ax_idx == 0:\n",
    "            method_handles.append(line)\n",
    "        if cat in limits:\n",
    "            ax.set_ylim(limits[cat])\n",
    "    ax.set_title(cat, fontsize=24)\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.set_xlabel('Num Steps', fontsize=16)\n",
    "\n",
    "labels = list(all_metrics.keys())\n",
    "# Create a custom handle for the pretrained dashed line\n",
    "plot_pretrained = True\n",
    "if plot_pretrained:\n",
    "    for ax_idx, (ax, cat) in enumerate(zip(axarr, cats)):\n",
    "        ax.axhline(pretrained_metrics[cat], color='black', linestyle='--')\n",
    "    pretrained_handle = Line2D([0], [0], color='black', linestyle='--', label='Pretrained')\n",
    "    labels.append('Pretrained')\n",
    "    method_handles.append(pretrained_handle)\n",
    "\n",
    "\n",
    "# Add global legend with method handles + pretrained handle\n",
    "fig.legend(handles=method_handles,\n",
    "           labels=labels,\n",
    "           loc='upper center', ncol=len(all_metrics) + 1, fontsize=16, bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseVisualizer:\n",
    "    def __init__(self,\n",
    "                 model_responses,\n",
    "                 questions,\n",
    "                 checkpoint_names,\n",
    "                 answers,\n",
    "                 pretrained_outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_responses: dict mapping model_name -> list of checkpoint_response_lists\n",
    "                             (EXCLUDING pretrained; these should correspond to checkpoint_names[1:])\n",
    "            questions:       list of N questions\n",
    "            checkpoint_names: list of len = 1 + num_checkpoints, e.g. ['pretrained','cp1','cp2',…]\n",
    "            answers:         list of N answers\n",
    "            pretrained_outputs: list of N strings, one “pretrained” response per question\n",
    "        \"\"\"\n",
    "        self.model_responses    = model_responses\n",
    "        self.questions          = questions\n",
    "        self.answers            = answers\n",
    "        self.pretrained_outputs = pretrained_outputs\n",
    "\n",
    "        self.model_names    = list(model_responses.keys())\n",
    "        self.num_models     = len(self.model_names)\n",
    "        self.checkpoint_names = checkpoint_names\n",
    "        # now includes pretrained as first row\n",
    "        self.num_checkpoints = len(checkpoint_names)\n",
    "        self.num_questions  = len(questions)\n",
    "\n",
    "    def wrap_text(self, text, width=80):\n",
    "        return '\\n'.join(textwrap.wrap(text, width=width))\n",
    "\n",
    "    def highlight_special_tokens(self, text):\n",
    "        # escape then re-insert span tags around <tokens>\n",
    "        escaped = text.replace('<','&lt;').replace('>','&gt;')\n",
    "        token_pattern = r'(&lt;/?[a-zA-Z_]+&gt;)'\n",
    "        return re.sub(\n",
    "            token_pattern,\n",
    "            lambda m: f'<span style=\"background-color:#fffa8c;'\n",
    "                      f'font-weight:bold; padding:2px 4px; border-radius:3px;\">'\n",
    "                      f'{m.group(1)}</span>',\n",
    "            escaped\n",
    "        ).replace('&lt;span','<span').replace('&lt;/span&gt;','</span>')\n",
    "\n",
    "    def display_responses_table(self, question_idx):\n",
    "        if not (0 <= question_idx < self.num_questions):\n",
    "            raise IndexError(f\"Question index must be 0 ≤ idx < {self.num_questions}\")\n",
    "\n",
    "        records = []\n",
    "        for model in self.model_names:\n",
    "            for cp_idx, cp_name in enumerate(self.checkpoint_names):\n",
    "                if cp_name == 'pretrained':\n",
    "                    # take from the separate list\n",
    "                    resp = self.pretrained_outputs[question_idx]\n",
    "                else:\n",
    "                    # offset by 1 because model_responses lists exclude pretrained\n",
    "                    model_cp_idx = cp_idx - 1\n",
    "                    try:\n",
    "                        resp = self.model_responses[model][model_cp_idx][question_idx]\n",
    "                    except Exception:\n",
    "                        resp = ''\n",
    "                html_resp = self.highlight_special_tokens(resp)\n",
    "                records.append({\n",
    "                    'Model': model,\n",
    "                    'Checkpoint': cp_name,\n",
    "                    'Response': html_resp\n",
    "                })\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        df_pivot = df.pivot(index='Checkpoint', columns='Model', values='Response')\n",
    "        # enforce ordering\n",
    "        df_pivot = df_pivot.reindex(self.checkpoint_names)\n",
    "\n",
    "        # header + answer\n",
    "        q_html = f\"\"\"\n",
    "        <div style=\"background:#eef5fa; padding:12px;\n",
    "                    border-left:6px solid #1a73e8; margin-bottom:15px;border-radius:4px;\">\n",
    "          <h3 style=\"margin:0;\">Q{question_idx}: {self.questions[question_idx]}</h3>\n",
    "        </div>\n",
    "        <div style=\"background:#fff8e1; padding:10px;\n",
    "                    border-left:6px solid #f39c12; margin-bottom:15px;border-radius:4px;\">\n",
    "          <strong>Answer:</strong> {self.answers[question_idx]}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "        styled = (\n",
    "            df_pivot.style\n",
    "                    .set_properties(**{\n",
    "                        'white-space':'pre-wrap',\n",
    "                        'text-align':'left',\n",
    "                        'vertical-align':'top',\n",
    "                        'padding':'8px',\n",
    "                        'line-height':'1.4'\n",
    "                    })\n",
    "                    .set_table_styles([\n",
    "                        {'selector':'th', \n",
    "                         'props':[\n",
    "                            ('background-color','#1a73e8'),\n",
    "                            ('color','white'),\n",
    "                            ('font-size','13px'),\n",
    "                            ('text-align','center'),\n",
    "                            ('padding','8px')\n",
    "                         ]},\n",
    "                        {'selector':'tr:nth-child(even)',\n",
    "                         'props':[('background-color','#f7f7f7')]},\n",
    "                        {'selector':'td',\n",
    "                         'props':[\n",
    "                            ('border','1px solid #ddd'),\n",
    "                            ('max-width','500px'),\n",
    "                            ('font-size','14px')\n",
    "                         ]}\n",
    "                    ])\n",
    "                    .set_caption(\"<b>Responses (models × checkpoints)</b>\")\n",
    "                    .set_uuid(\"\")  # stable HTML\n",
    "                    .to_html(escape=False)\n",
    "        )\n",
    "\n",
    "        return HTML(q_html + styled), df_pivot\n",
    "\n",
    "    def interactive_visualizer(self):\n",
    "        slider   = IntSlider(value=0, min=0, max=self.num_questions-1,\n",
    "                             step=1, description='Question:',\n",
    "                             continuous_update=False,\n",
    "                             style={'description_width':'initial'})\n",
    "        out      = Output()\n",
    "        header   = HTMLWidget(value=f\"<h2>Response Visualizer ({self.num_models} models, \"\n",
    "                                    f\"{self.num_checkpoints} checkpoints)</h2>\")\n",
    "        save_btn = Button(description='Save as PNG')\n",
    "\n",
    "        def update(idx):\n",
    "            with out:\n",
    "                out.clear_output(wait=True)\n",
    "                table_html, df_pivot = self.display_responses_table(idx)\n",
    "                display(table_html)\n",
    "\n",
    "                # fig_len = self.display_length_comparison(idx)\n",
    "                # plt.show()\n",
    "\n",
    "                # ta = self.display_token_analysis(idx)\n",
    "                # if hasattr(ta, '__html__'):\n",
    "                #     display(ta)\n",
    "                # else:\n",
    "                #     plt.show()\n",
    "\n",
    "                # stash for saving\n",
    "                self._last_df   = df_pivot\n",
    "                # self._last_figs = [fig_len]\n",
    "                # tok_fig = self.display_token_analysis(idx)\n",
    "                # if not hasattr(tok_fig, '__html__'):\n",
    "                #     self._last_figs.append(tok_fig)\n",
    "\n",
    "        def save_snapshot(_):\n",
    "            idx = slider.value\n",
    "            # --- 1) Render table as a Matplotlib figure and save ---\n",
    "            fig_table, ax = plt.subplots(figsize=(12, 8))\n",
    "            ax.axis('off')\n",
    "            # Build wrapped cell text\n",
    "            cell_text = [\n",
    "                [\n",
    "                  textwrap.fill(self._last_df.loc[cp, model], width=40)\n",
    "                  for model in self._last_df.columns\n",
    "                ]\n",
    "                for cp in self._last_df.index\n",
    "            ]\n",
    "            tbl = ax.table(\n",
    "                cellText=cell_text,\n",
    "                rowLabels=self._last_df.index.tolist(),\n",
    "                colLabels=self._last_df.columns.tolist(),\n",
    "                cellLoc='left',\n",
    "                loc='center'\n",
    "            )\n",
    "            tbl.auto_set_font_size(False)\n",
    "            tbl.set_fontsize(10)\n",
    "            fig_table.tight_layout()\n",
    "            fig_table.savefig(f\"question_{idx}_table.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig_table)\n",
    "\n",
    "            # --- 2) Save all stored Matplotlib figures ---\n",
    "            for i, fig in enumerate(self._last_figs):\n",
    "                fig.savefig(f\"question_{idx}_fig{i}.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "\n",
    "            print(f\"Saved: question_{idx}_table.png + {len(self._last_figs)} figure(s).\")\n",
    "\n",
    "        save_btn.on_click(save_snapshot)\n",
    "        interact(update, idx=slider)\n",
    "        controls = HBox([slider, save_btn], layout={'align_items':'center','spacing':'20px'})\n",
    "        return VBox([header, controls, out])\n",
    "    \n",
    "    def count_special_tokens(self, text):\n",
    "        \"\"\"\n",
    "        Count special tokens (like <reasoning>) in the text.\n",
    "        \n",
    "        Args:\n",
    "            text: The response text that may contain special tokens.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of token counts\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Define pattern for tokens like <token_name> and </token_name>\n",
    "        token_pattern = r'</?([a-zA-Z_]+)>'\n",
    "        \n",
    "        # Find all tokens\n",
    "        tokens = re.findall(token_pattern, text)\n",
    "        \n",
    "        # Count token occurrences\n",
    "        token_counts = {}\n",
    "        for token in tokens:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "        \n",
    "        return token_counts\n",
    "    \n",
    "    def display_length_comparison(self, question_idx):\n",
    "        \"\"\"\n",
    "        Display a bar chart comparing response lengths across models and checkpoints.\n",
    "        \n",
    "        Args:\n",
    "            question_idx: Index of the question to display response lengths for.\n",
    "        \"\"\"\n",
    "        if not 0 <= question_idx < self.num_questions:\n",
    "            print(f\"Question index must be between 0 and {self.num_questions-1}\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        bar_width = 0.8 / self.num_models\n",
    "        checkpoint_positions = np.arange(self.num_checkpoints)\n",
    "        \n",
    "        for i, model_name in enumerate(self.model_names):\n",
    "            response_lengths = [len(self.model_responses[model_name][cp][question_idx]) \n",
    "                               for cp in range(self.num_checkpoints)]\n",
    "            \n",
    "            positions = checkpoint_positions + (i * bar_width)\n",
    "            ax.bar(positions, response_lengths, width=bar_width, label=model_name)\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Checkpoint')\n",
    "        ax.set_ylabel('Response Length (characters)')\n",
    "        ax.set_title(f'Response Length Comparison for Question {question_idx}')\n",
    "        ax.set_xticks(checkpoint_positions + bar_width * (self.num_models - 1) / 2)\n",
    "        ax.set_xticklabels(self.checkpoint_names, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def display_token_analysis(self, question_idx):\n",
    "        \"\"\"\n",
    "        Display analysis of special tokens in responses.\n",
    "        \n",
    "        Args:\n",
    "            question_idx: Index of the question to analyze tokens for.\n",
    "        \"\"\"\n",
    "        if not 0 <= question_idx < self.num_questions:\n",
    "            print(f\"Question index must be between 0 and {self.num_questions-1}\")\n",
    "            return\n",
    "        \n",
    "        # Collect token counts for all models and checkpoints\n",
    "        all_tokens = set()\n",
    "        token_data = {}\n",
    "        \n",
    "        for model_name in self.model_names:\n",
    "            token_data[model_name] = []\n",
    "            \n",
    "            for cp in range(self.num_checkpoints):\n",
    "                response = self.model_responses[model_name][cp][question_idx]\n",
    "                token_counts = self.count_special_tokens(response)\n",
    "                token_data[model_name].append(token_counts)\n",
    "                all_tokens.update(token_counts.keys())\n",
    "        \n",
    "        # Create plots for token analysis\n",
    "        if not all_tokens:\n",
    "            # No tokens found\n",
    "            return HTML(\"<div style='padding: 10px; background-color: #f8f9fa; border-left: 5px solid #6c757d;'><p>No special tokens detected in the responses for this question.</p></div>\")\n",
    "        \n",
    "        # Sort tokens alphabetically\n",
    "        all_tokens = sorted(list(all_tokens))\n",
    "        \n",
    "        # Create a figure with subplots for each token\n",
    "        n_tokens = len(all_tokens)\n",
    "        fig, axes = plt.subplots(nrows=n_tokens, figsize=(14, 4*n_tokens))\n",
    "        \n",
    "        # Handle the case when there's only one token\n",
    "        if n_tokens == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        # Get bar positions\n",
    "        bar_width = 0.8 / self.num_models\n",
    "        checkpoint_positions = np.arange(self.num_checkpoints)\n",
    "        \n",
    "        for token_idx, token in enumerate(all_tokens):\n",
    "            ax = axes[token_idx]\n",
    "            \n",
    "            # Prepare data\n",
    "            for i, model_name in enumerate(self.model_names):\n",
    "                token_counts = [token_data[model_name][cp].get(token, 0) for cp in range(self.num_checkpoints)]\n",
    "                \n",
    "                positions = checkpoint_positions + (i * bar_width)\n",
    "                ax.bar(positions, token_counts, width=bar_width, label=model_name if token_idx == 0 else \"\")\n",
    "            \n",
    "            # Add title and labels\n",
    "            ax.set_title(f'Token <{token}> Frequency')\n",
    "            ax.set_xlabel('Checkpoint')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_xticks(checkpoint_positions + bar_width * (self.num_models - 1) / 2)\n",
    "            ax.set_xticklabels(self.checkpoint_names, rotation=45, ha='right')\n",
    "            \n",
    "            # Add grid for readability\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add legend to the first subplot only\n",
    "        if n_tokens > 0:\n",
    "            axes[0].legend(loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "\n",
    "    \n",
    "# If running in a Jupyter notebook environment, this will display the interactive visualizer\n",
    "questions = [dataset[i]['question'] for i in range(100)]\n",
    "answers = [dataset[i]['answer'] for i in range(100)]\n",
    "checkpoint_names = ['pretrained'] + checkpoints\n",
    "visualizer = ResponseVisualizer(responses, questions, checkpoint_names, answers, pretrained_outputs)\n",
    "visualizer.interactive_visualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

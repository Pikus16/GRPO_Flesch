{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe_image as dfi\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact, IntSlider, Output, VBox, HTML as HTMLWidget, Button, HBox\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "import textstat\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from glob import glob\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    return text.split(\"####\")[-1].strip() if \"####\" in text else None\n",
    "\n",
    "def get_gsm8k_questions(split=\"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_gsm8k_questions(split = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = glob('lora_checkpoints/*/test_examples.json')\n",
    "responses = {}\n",
    "for o in output_files:\n",
    "    with open(o) as f:\n",
    "        all_outputs = json.load(f)\n",
    "    assert len(all_outputs) == 7\n",
    "    responses[o.split('/')[1]] = all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(text: str) -> str:\n",
    "    return text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "\n",
    "def get_answer_rate(responses):\n",
    "    correct, wrong = 0, 0\n",
    "    for i, o in enumerate(responses):\n",
    "        try: \n",
    "            ans = int(get_answer(o))\n",
    "            if ans == int(dataset[i]['answer']):\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        except:\n",
    "            pass\n",
    "    return correct / (wrong + correct)\n",
    "\n",
    "def get_total_average_flesch_kincaid(responses) -> float:\n",
    "    scores = [textstat.flesch_kincaid_grade(r) for r in responses]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def flesch_kincaid_reward_func(responses) -> float:\n",
    "    scores = [textstat.flesch_kincaid_grade(r.split('<reasoning>')[-1].split('</reasoning>')[0]) for r in responses]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def get_average_length(responses) -> list[float]:\n",
    "    return sum([len(r) for r in responses]) / len(responses)\n",
    "\n",
    "def soft_format_reward_func(responses) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
    "    return sum([1 if match else 0 for match in matches])\n",
    "\n",
    "def has_reasoning(responses) -> list[float]:\n",
    "    return sum([1 if (\"<reasoning>\" in r and '</reasoning>' in r) else 0 for r in responses])\n",
    "\n",
    "def has_answer(responses) -> list[float]:\n",
    "    return sum([1 if (\"<answer>\" in r and \"</answer>\" in r) else 0 for r in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {}\n",
    "for n, all_outputs in responses.items():\n",
    "    metrics = []\n",
    "    for outputs in all_outputs:\n",
    "        metrics.append({\n",
    "            \"correct answer\\n(when <answer> present)\": get_answer_rate(outputs),\n",
    "            \"average_flesch_kincaid\": get_total_average_flesch_kincaid(outputs),\n",
    "            \"reasoning flesch kincaid\": flesch_kincaid_reward_func(outputs),\n",
    "            \"average_length\": get_average_length(outputs),\n",
    "            \"soft_format_reward\": soft_format_reward_func(outputs),\n",
    "            \"has reasoning tokens\": has_reasoning(outputs),\n",
    "            \"has answer tokens\": has_answer(outputs),\n",
    "        })\n",
    "    all_metrics[n] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in responses.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "checkpoints = list(range(500, 3001, 500))\n",
    "fig, axarr = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axarr = axarr.flatten()\n",
    "cats = metrics[0].keys()\n",
    "colors = ['r', 'b', 'g']\n",
    "\n",
    "# Collect handles for method lines only from the first subplot\n",
    "method_handles = []\n",
    "\n",
    "for ax_idx, (ax, cat) in enumerate(zip(axarr, cats)):\n",
    "    for i, (name, metrics) in enumerate(all_metrics.items()):\n",
    "        y = [metrics[i][cat] for i in range(1, len(all_outputs))]\n",
    "        line, = ax.plot(checkpoints, y, marker='o', color=colors[i], label=name)\n",
    "        ax.axhline(metrics[0][cat], color=colors[i], linestyle='--')\n",
    "        if ax_idx == 0:\n",
    "            method_handles.append(line)\n",
    "    ax.set_title(cat, fontsize=24)\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.set_xlabel('Num Steps', fontsize=16)\n",
    "\n",
    "# Create a custom handle for the pretrained dashed line\n",
    "pretrained_handle = Line2D([0], [0], color='black', linestyle='--', label='Pretrained')\n",
    "\n",
    "# Add global legend with method handles + pretrained handle\n",
    "fig.legend(handles=method_handles + [pretrained_handle],\n",
    "           labels=list(all_metrics.keys()) + ['Pretrained'],\n",
    "           loc='upper center', ncol=len(all_metrics) + 1, fontsize=16, bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "i_ = np.random.randint(len(all_outputs[0]), size=3)\n",
    "response_dict = {}\n",
    "answer_dict = {}\n",
    "llm_names = ['pretrained'] + [str(s) for s in checkpoints]\n",
    "for i in i_:\n",
    "    i = int(i)\n",
    "    question = dataset[i]['question']\n",
    "    answer = dataset[i]['answer']\n",
    "    response_dict[question] = [\n",
    "        all_outputs[j][i] for j in range(len(all_outputs))\n",
    "    ]\n",
    "    answer_dict[question] = answer\n",
    "\n",
    "for j, (k,vs) in enumerate(response_dict.items()):\n",
    "    if j != 2:\n",
    "        continue\n",
    "    print('QUESTION:')\n",
    "    wrapped_text = textwrap.fill(k, width=150)\n",
    "    print(wrapped_text)\n",
    "    print(f'ANSWER = {answer_dict[k]}')\n",
    "    print('-------------')\n",
    "    for i,v in enumerate(vs):\n",
    "        wrapped_text = textwrap.fill(v, width=150)\n",
    "        print(f'RESPONSE from {llm_names[i]}')\n",
    "        print(wrapped_text)\n",
    "        print('---------')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseVisualizer:\n",
    "    def __init__(self, model_responses, questions, checkpoint_names, answers):\n",
    "        \"\"\"\n",
    "        Initialize the visualizer with model responses and questions.\n",
    "        \n",
    "        Args:\n",
    "            model_responses: Dictionary with model names as keys and lists of checkpoint responses as values.\n",
    "                             Each checkpoint contains responses to 100 questions.\n",
    "            questions: List of 100 questions corresponding to the responses.\n",
    "        \"\"\"\n",
    "        self.model_responses = model_responses\n",
    "        self.questions = questions\n",
    "        self.num_models = len(model_responses)\n",
    "        self.model_names = list(model_responses.keys())\n",
    "        self.num_checkpoints = len(model_responses[self.model_names[0]])\n",
    "        self.num_questions = len(questions)\n",
    "        self.checkpoint_names = checkpoint_names\n",
    "        self.answers = answers\n",
    "    \n",
    "    def wrap_text(self, text, width=80):\n",
    "        \"\"\"Wraps text to specified width.\"\"\"\n",
    "        return '\\n'.join(textwrap.wrap(text, width=width))\n",
    "    \n",
    "    def highlight_special_tokens(self, text):\n",
    "        \"\"\"\n",
    "        Highlight special tokens (like <reasoning>) in the text.\n",
    "        \n",
    "        Args:\n",
    "            text: The response text that may contain special tokens.\n",
    "            \n",
    "        Returns:\n",
    "            HTML formatted text with highlighted tokens.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Define pattern for tokens like <token_name>\n",
    "        token_pattern = r'(<[a-zA-Z_]+>|</[a-zA-Z_]+>)'\n",
    "        \n",
    "        # Replace tokens with highlighted version\n",
    "        highlighted_text = re.sub(\n",
    "            token_pattern,\n",
    "            r'<span style=\"background-color: #ffff99; font-weight: bold; padding: 2px 4px; border-radius: 3px;\">\\1</span>',\n",
    "            text.replace('<', '&lt;').replace('>', '&gt;')\n",
    "        )\n",
    "        \n",
    "        return highlighted_text.replace('&lt;span', '<span').replace('&lt;/span&gt;', '</span>')\n",
    "    \n",
    "    def display_responses_table(self, question_idx):\n",
    "        \"\"\"\n",
    "        Display responses in a 3×7 HTML table (models × checkpoints) for better readability.\n",
    "        \"\"\"\n",
    "        if not 0 <= question_idx < self.num_questions:\n",
    "            print(f\"Question index must be between 0 and {self.num_questions-1}\")\n",
    "            return\n",
    "        \n",
    "        # Build flat list\n",
    "        records = []\n",
    "        for model_name in self.model_names:\n",
    "            for cp_idx, cp_name in enumerate(self.checkpoint_names):\n",
    "                resp = self.model_responses[model_name][cp_idx][question_idx]\n",
    "                # highlight tokens & wrap long lines\n",
    "                #html_resp = self.highlight_special_tokens(self.wrap_text(resp, width=60))\n",
    "                html_resp = self.highlight_special_tokens(resp)\n",
    "                \n",
    "                records.append({\n",
    "                    'Model': model_name,\n",
    "                    'Checkpoint': cp_name,\n",
    "                    'Response': html_resp\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "\n",
    "        # Pivot so rows=checkpoints, cols=models\n",
    "        df_pivot = df.pivot(index='Checkpoint', columns='Model', values='Response')\n",
    "\n",
    "        # ---- new: enforce the exact row order ----\n",
    "        df_pivot = df_pivot.reindex(self.checkpoint_names)\n",
    "        \n",
    "        # Display question header\n",
    "        q_html = f\"\"\"\n",
    "        <div style=\"background-color: #eef5fa; padding: 12px; margin-bottom: 15px; \n",
    "                    border-radius: 4px; border-left: 6px solid #1a73e8;\">\n",
    "          <h3 style=\"margin:0;\">Question {question_idx}:</h3>\n",
    "          <p style=\"margin:4px 0 0; font-size:15px;\">{self.questions[question_idx]}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "        answer_html = f\"\"\"\n",
    "        <div style=\"background-color: #fff8e1; padding: 10px; margin-bottom: 15px;\n",
    "                    border-radius: 4px; border-left: 6px solid #f39c12;\">\n",
    "            <h4 style=\"margin:0;\">Answer:</h4>\n",
    "            <p style=\"margin:4px 0 0; font-size:15px;\">{self.answers[question_idx]}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        q_html += answer_html\n",
    "        \n",
    "        # Style the pivoted table\n",
    "        styled = (df_pivot.style\n",
    "                  .set_properties(**{\n",
    "                      'white-space': 'pre-wrap',\n",
    "                      'text-align': 'left',\n",
    "                      'vertical-align': 'top',\n",
    "                      'padding': '8px',\n",
    "                      'line-height': '1.4'\n",
    "                  })\n",
    "                  .set_table_styles([\n",
    "                      {'selector': 'th',\n",
    "                       'props': [\n",
    "                           ('background-color', '#1a73e8'),\n",
    "                           ('color', 'white'),\n",
    "                           ('font-size', '13px'),\n",
    "                           ('text-align', 'center'),\n",
    "                           ('padding', '8px')\n",
    "                       ]},\n",
    "                      {'selector': 'tr:nth-child(even)',\n",
    "                       'props': [('background-color', '#f7f7f7')]},\n",
    "                      {'selector': 'td',\n",
    "                       'props': [\n",
    "                           ('border', '1px solid #ddd'),\n",
    "                           ('max-width', '500px'),\n",
    "                           ('font-size', '14px')\n",
    "                       ]}\n",
    "                  ])\n",
    "                  .set_caption(f\"<b>Responses (models × checkpoints)</b>\")\n",
    "                  .set_uuid(\"\")  # to avoid random IDs in Notebook\n",
    "                  .to_html(escape=False)  # IMPORTANT: allow your HTML in cells\n",
    "                  )\n",
    "        \n",
    "        return HTML(q_html + styled), df_pivot \n",
    "\n",
    "    def interactive_visualizer(self):\n",
    "        slider   = IntSlider(value=0, min=0, max=self.num_questions-1,\n",
    "                             step=1, description='Question:',\n",
    "                             continuous_update=False,\n",
    "                             style={'description_width':'initial'})\n",
    "        out      = Output()\n",
    "        header   = HTMLWidget(value=f\"<h2>Response Visualizer ({self.num_models} models, \"\n",
    "                                    f\"{self.num_checkpoints} checkpoints)</h2>\")\n",
    "        save_btn = Button(description='Save as PNG')\n",
    "\n",
    "        def update(idx):\n",
    "            with out:\n",
    "                out.clear_output(wait=True)\n",
    "                table_html, df_pivot = self.display_responses_table(idx)\n",
    "                display(table_html)\n",
    "\n",
    "                fig_len = self.display_length_comparison(idx)\n",
    "                plt.show()\n",
    "\n",
    "                ta = self.display_token_analysis(idx)\n",
    "                if hasattr(ta, '__html__'):\n",
    "                    display(ta)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "                # stash for saving\n",
    "                self._last_df   = df_pivot\n",
    "                self._last_figs = [fig_len]\n",
    "                tok_fig = self.display_token_analysis(idx)\n",
    "                if not hasattr(tok_fig, '__html__'):\n",
    "                    self._last_figs.append(tok_fig)\n",
    "\n",
    "        def save_snapshot(_):\n",
    "            idx = slider.value\n",
    "            # --- 1) Render table as a Matplotlib figure and save ---\n",
    "            fig_table, ax = plt.subplots(figsize=(12, 8))\n",
    "            ax.axis('off')\n",
    "            # Build wrapped cell text\n",
    "            cell_text = [\n",
    "                [\n",
    "                  textwrap.fill(self._last_df.loc[cp, model], width=40)\n",
    "                  for model in self._last_df.columns\n",
    "                ]\n",
    "                for cp in self._last_df.index\n",
    "            ]\n",
    "            tbl = ax.table(\n",
    "                cellText=cell_text,\n",
    "                rowLabels=self._last_df.index.tolist(),\n",
    "                colLabels=self._last_df.columns.tolist(),\n",
    "                cellLoc='left',\n",
    "                loc='center'\n",
    "            )\n",
    "            tbl.auto_set_font_size(False)\n",
    "            tbl.set_fontsize(10)\n",
    "            fig_table.tight_layout()\n",
    "            fig_table.savefig(f\"question_{idx}_table.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close(fig_table)\n",
    "\n",
    "            # --- 2) Save all stored Matplotlib figures ---\n",
    "            for i, fig in enumerate(self._last_figs):\n",
    "                fig.savefig(f\"question_{idx}_fig{i}.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "\n",
    "            print(f\"Saved: question_{idx}_table.png + {len(self._last_figs)} figure(s).\")\n",
    "\n",
    "        save_btn.on_click(save_snapshot)\n",
    "        interact(update, idx=slider)\n",
    "        controls = HBox([slider, save_btn], layout={'align_items':'center','spacing':'20px'})\n",
    "        return VBox([header, controls, out])\n",
    "    \n",
    "    def count_special_tokens(self, text):\n",
    "        \"\"\"\n",
    "        Count special tokens (like <reasoning>) in the text.\n",
    "        \n",
    "        Args:\n",
    "            text: The response text that may contain special tokens.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of token counts\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Define pattern for tokens like <token_name> and </token_name>\n",
    "        token_pattern = r'</?([a-zA-Z_]+)>'\n",
    "        \n",
    "        # Find all tokens\n",
    "        tokens = re.findall(token_pattern, text)\n",
    "        \n",
    "        # Count token occurrences\n",
    "        token_counts = {}\n",
    "        for token in tokens:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "        \n",
    "        return token_counts\n",
    "    \n",
    "    def display_length_comparison(self, question_idx):\n",
    "        \"\"\"\n",
    "        Display a bar chart comparing response lengths across models and checkpoints.\n",
    "        \n",
    "        Args:\n",
    "            question_idx: Index of the question to display response lengths for.\n",
    "        \"\"\"\n",
    "        if not 0 <= question_idx < self.num_questions:\n",
    "            print(f\"Question index must be between 0 and {self.num_questions-1}\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        bar_width = 0.8 / self.num_models\n",
    "        checkpoint_positions = np.arange(self.num_checkpoints)\n",
    "        \n",
    "        for i, model_name in enumerate(self.model_names):\n",
    "            response_lengths = [len(self.model_responses[model_name][cp][question_idx]) \n",
    "                               for cp in range(self.num_checkpoints)]\n",
    "            \n",
    "            positions = checkpoint_positions + (i * bar_width)\n",
    "            ax.bar(positions, response_lengths, width=bar_width, label=model_name)\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Checkpoint')\n",
    "        ax.set_ylabel('Response Length (characters)')\n",
    "        ax.set_title(f'Response Length Comparison for Question {question_idx}')\n",
    "        ax.set_xticks(checkpoint_positions + bar_width * (self.num_models - 1) / 2)\n",
    "        ax.set_xticklabels(self.checkpoint_names, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def display_token_analysis(self, question_idx):\n",
    "        \"\"\"\n",
    "        Display analysis of special tokens in responses.\n",
    "        \n",
    "        Args:\n",
    "            question_idx: Index of the question to analyze tokens for.\n",
    "        \"\"\"\n",
    "        if not 0 <= question_idx < self.num_questions:\n",
    "            print(f\"Question index must be between 0 and {self.num_questions-1}\")\n",
    "            return\n",
    "        \n",
    "        # Collect token counts for all models and checkpoints\n",
    "        all_tokens = set()\n",
    "        token_data = {}\n",
    "        \n",
    "        for model_name in self.model_names:\n",
    "            token_data[model_name] = []\n",
    "            \n",
    "            for cp in range(self.num_checkpoints):\n",
    "                response = self.model_responses[model_name][cp][question_idx]\n",
    "                token_counts = self.count_special_tokens(response)\n",
    "                token_data[model_name].append(token_counts)\n",
    "                all_tokens.update(token_counts.keys())\n",
    "        \n",
    "        # Create plots for token analysis\n",
    "        if not all_tokens:\n",
    "            # No tokens found\n",
    "            return HTML(\"<div style='padding: 10px; background-color: #f8f9fa; border-left: 5px solid #6c757d;'><p>No special tokens detected in the responses for this question.</p></div>\")\n",
    "        \n",
    "        # Sort tokens alphabetically\n",
    "        all_tokens = sorted(list(all_tokens))\n",
    "        \n",
    "        # Create a figure with subplots for each token\n",
    "        n_tokens = len(all_tokens)\n",
    "        fig, axes = plt.subplots(nrows=n_tokens, figsize=(14, 4*n_tokens))\n",
    "        \n",
    "        # Handle the case when there's only one token\n",
    "        if n_tokens == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        # Get bar positions\n",
    "        bar_width = 0.8 / self.num_models\n",
    "        checkpoint_positions = np.arange(self.num_checkpoints)\n",
    "        \n",
    "        for token_idx, token in enumerate(all_tokens):\n",
    "            ax = axes[token_idx]\n",
    "            \n",
    "            # Prepare data\n",
    "            for i, model_name in enumerate(self.model_names):\n",
    "                token_counts = [token_data[model_name][cp].get(token, 0) for cp in range(self.num_checkpoints)]\n",
    "                \n",
    "                positions = checkpoint_positions + (i * bar_width)\n",
    "                ax.bar(positions, token_counts, width=bar_width, label=model_name if token_idx == 0 else \"\")\n",
    "            \n",
    "            # Add title and labels\n",
    "            ax.set_title(f'Token <{token}> Frequency')\n",
    "            ax.set_xlabel('Checkpoint')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.set_xticks(checkpoint_positions + bar_width * (self.num_models - 1) / 2)\n",
    "            ax.set_xticklabels(self.checkpoint_names, rotation=45, ha='right')\n",
    "            \n",
    "            # Add grid for readability\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add legend to the first subplot only\n",
    "        if n_tokens > 0:\n",
    "            axes[0].legend(loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# If running in a Jupyter notebook environment, this will display the interactive visualizer\n",
    "questions = [dataset[i]['question'] for i in range(100)]\n",
    "answers = [dataset[i]['answer'] for i in range(100)]\n",
    "checkpoint_names = ['pretrained'] + checkpoints\n",
    "visualizer = ResponseVisualizer(responses, questions, checkpoint_names, answers)\n",
    "visualizer.interactive_visualizer()\n",
    "\n",
    "# For non-interactive environments, you can directly view specific questions\n",
    "# visualizer.display_responses_table(0)  # Display responses for question 0\n",
    "# visualizer.display_length_comparison(0)  # Show length comparison for question 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

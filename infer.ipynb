{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from unsloth import FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "import textstat\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"device_map\": \"auto\",  # Automatically handle model placement\n",
    "    \"torch_dtype\": torch.float16,  # Use half precision\n",
    "    \"use_cache\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\"unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit\", **params).to(\"cuda\").eval()\n",
    "pretrained_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit\")\n",
    "\n",
    "model_lora = AutoModelForCausalLM.from_pretrained(\"/home/ben/code/wandb/gsm8k/test_gsm8k/merged\", **params).to(\"cuda\").eval()\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(\"/home/ben/code/wandb/gsm8k/test_gsm8k/merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "output_pretrained, output_lora = [], []\n",
    "num_ex = 50\n",
    "for i in tqdm(range(0, num_ex, BATCH_SIZE)):\n",
    "    batch_end = min(i + BATCH_SIZE, num_ex)\n",
    "    batch_prompts = []\n",
    "    \n",
    "    # Prepare batch of prompts\n",
    "    for j in range(i, batch_end):\n",
    "        prompt = dataset[j]['question']\n",
    "        answer = dataset[j]['answer']\n",
    "        batch_prompts.append([\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ])\n",
    "    text = [pretrained_tokenizer.apply_chat_template(prompt, tokenize = False, add_generation_prompt = True) for prompt in batch_prompts]\n",
    "\n",
    "    inputs = pretrained_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "    input_length = inputs.input_ids.shape[-1]\n",
    "    output = pretrained_model.generate(**inputs, **generation_config )\n",
    "    output = output[:, input_length: ]\n",
    "    batch_outputs = pretrained_tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    output_pretrained.extend(batch_outputs)\n",
    "\n",
    "    output = model_lora.generate(**inputs, **generation_config)\n",
    "    output = output[:, input_length: ]\n",
    "    batch_outputs = tokenizer_lora.batch_decode(output, skip_special_tokens=True)\n",
    "    output_lora.extend(batch_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(text: str) -> str:\n",
    "    return text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "\n",
    "def get_answer_rate(responses):\n",
    "    correct, wrong = 0, 0\n",
    "    for i, o in enumerate(responses):\n",
    "        try: \n",
    "            ans = int(get_answer(o))\n",
    "            if ans == int(dataset[i]['answer']):\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        except:\n",
    "            pass\n",
    "    return correct, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer_rate(output_pretrained), get_answer_rate(output_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_flesch_kincaid(responses) -> list[float]:\n",
    "    scores = [textstat.flesch_kincaid_grade(r) for r in responses]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "get_average_flesch_kincaid(output_pretrained), get_average_flesch_kincaid(output_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_length(responses) -> list[float]:\n",
    "    return sum([len(r) for r in responses]) / len(responses)\n",
    "\n",
    "get_average_length(output_pretrained), get_average_length(output_lora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_format_reward_func(responses) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
    "    return sum([1 if match else 0 for match in matches])\n",
    "\n",
    "soft_format_reward_func(output_lora), soft_format_reward_func(output_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_reasoning(responses) -> list[float]:\n",
    "    return sum([1 if \"<reasoning>\" in r else 0 for r in responses])\n",
    "\n",
    "has_reasoning(output_lora), has_reasoning(output_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_answer(responses) -> list[float]:\n",
    "    return sum([1 if \"<answer>\" in r else 0 for r in responses])\n",
    "\n",
    "has_answer(output_lora), has_answer(output_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

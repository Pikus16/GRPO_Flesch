{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from unsloth import FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "import textstat\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"device_map\": \"auto\",  # Automatically handle model placement\n",
    "    \"torch_dtype\": torch.float16,  # Use half precision\n",
    "    \"use_cache\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    'unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit',\n",
    "    \"/home/ben/code/wandb/gsm8k/test_gsm8k/merged\",\n",
    "    \"/home/ben/code/wandb/gsm8k/gsm8k_flesch/merged\",\n",
    "    '/home/ben/code/wandb/gsm8k/outputs/checkpoint-1500',\n",
    "    '/home/ben/code/wandb/gsm8k/outputs/checkpoint-2500',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions(split = \"test\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path: str):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, **params).to(\"cuda\").eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, **params)\n",
    "    return model, tokenizer\n",
    "\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "def run_generation(model_, tokenizer_, batch_size=8, num_ex=50):\n",
    "    outputs = []\n",
    "    for i in tqdm(range(0, num_ex, batch_size)):\n",
    "        batch_end = min(i + batch_size, num_ex)\n",
    "        batch_prompts = []\n",
    "        \n",
    "        # Prepare batch of prompts\n",
    "        for j in range(i, batch_end):\n",
    "            prompt = dataset[j]['question']\n",
    "            answer = dataset[j]['answer']\n",
    "            batch_prompts.append([\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ])\n",
    "        text = [tokenizer_.apply_chat_template(prompt, tokenize = False, add_generation_prompt = True) for prompt in batch_prompts]\n",
    "\n",
    "        inputs = tokenizer_(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "        input_length = inputs.input_ids.shape[-1]\n",
    "        output = model_.generate(**inputs, **generation_config )\n",
    "        output = output[:, input_length: ]\n",
    "        batch_outputs = tokenizer_.batch_decode(output, skip_special_tokens=True)\n",
    "        outputs.extend(batch_outputs)\n",
    "    return outputs\n",
    "\n",
    "def run(model_path: str):\n",
    "    model, tokenizer = load_model(model_path)\n",
    "    outputs = run_generation(model, tokenizer)\n",
    "    model.to(\"cpu\")\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "for model_path in model_paths:\n",
    "    outputs = run(model_path)\n",
    "    all_outputs.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(text: str) -> str:\n",
    "    return text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "\n",
    "def get_answer_rate(responses):\n",
    "    correct, wrong = 0, 0\n",
    "    for i, o in enumerate(responses):\n",
    "        try: \n",
    "            ans = int(get_answer(o))\n",
    "            if ans == int(dataset[i]['answer']):\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        except:\n",
    "            pass\n",
    "    return correct, wrong\n",
    "\n",
    "def get_average_flesch_kincaid(responses) -> list[float]:\n",
    "    scores = [textstat.flesch_kincaid_grade(r) for r in responses]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def get_average_length(responses) -> list[float]:\n",
    "    return sum([len(r) for r in responses]) / len(responses)\n",
    "\n",
    "def soft_format_reward_func(responses) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
    "    return sum([1 if match else 0 for match in matches])\n",
    "\n",
    "def has_reasoning(responses) -> list[float]:\n",
    "    return sum([1 if \"<reasoning>\" in r else 0 for r in responses])\n",
    "\n",
    "def has_answer(responses) -> list[float]:\n",
    "    return sum([1 if \"<answer>\" in r else 0 for r in responses])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for model_path, outputs in zip(model_paths, all_outputs):\n",
    "    metrics.append({\n",
    "        'model_path': model_path,\n",
    "        \"answer_rate\": get_answer_rate(outputs),\n",
    "        \"average_flesch_kincaid\": get_average_flesch_kincaid(outputs),\n",
    "        \"average_length\": get_average_length(outputs),\n",
    "        \"soft_format_reward\": soft_format_reward_func(outputs),\n",
    "        \"has_reasoning\": has_reasoning(outputs),\n",
    "        \"has_answer\": has_answer(outputs),\n",
    "    })\n",
    "pd.DataFrame(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
